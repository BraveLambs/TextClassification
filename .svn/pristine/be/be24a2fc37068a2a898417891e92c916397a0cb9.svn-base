import pickle
import gensim
from gensim import corpora
import pynlpir
import os
from sklearn.utils import Bunch

def clean(raw_data_input_path, word_bag_filepath):
    '''
    函数说明: 对下载的源新闻文档进行分词处理，并且从分词结果中仅取名词
    :param raw_data_input_path: 源文档根目录
    :param word_bag_filepath: 将清洗的结果bunch持久化到词袋文件
    :return: 
    '''
    bunch = Bunch(target_name=[], label=[], filenames=[], contents=[])

    seg_data_output_head_path = 'F:\\seg\\'
    catagorys = os.listdir(raw_data_input_path)
    bunch.target_name.extend(catagorys)
    pynlpir.open()
    for catagory in catagorys:
        catagory_path = os.path.join(raw_data_input_path, catagory)
        # new_catagory_path = os.path.join(seg_data_output_head_path, catagory)
        # if not (os.path.exits(new_catagory_path)):
        #     os.mkdir(new_catagory_path)
        raw_documents = os.listdir(catagory_path)
        for document in raw_documents:
            document_path = os.path.join(catagory_path, document)
            # 对每篇文章进行分词处理
            f = open(document_path, 'r')
            p = f.read()
            segments = pynlpir.segment(p, pos_english=True)
            # 对分词结果取名词
            seg_only_noun = [element[0] for element in segments if element[1] == 'noun']
            document_cleaned = ' '.join(seg_only_noun)
            bunch.filenames.append(document)
            bunch.label.append(catagory)
            bunch.contents.append(document_cleaned)
    pynlpir.close()
    with open(word_bag_filepath, 'wb') as file_obj:
        pickle.dump(bunch, file_obj)

if __name__ == '__main__':
    # 清洗模块测试
    raw_data_input_path = 'F:\\train'
    word_bag_filepath = 'F:\\result\\word_bag.dat'
    # clean(raw_data_input_path, word_bag_filepath)
    # 读取bunch
    with open(word_bag_filepath, "rb") as file_obj:
        bunch = pickle.load(file_obj)
    # print(bunch.target_name)
    # for t in bunch.contents:
    #     print(t)
    word = [doc.split(',') for doc in bunch.contents]
    dictionary = corpora.Dictionary(word)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in word]
    # for t in doc_term_matrix:
    #     print(t)
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(doc_term_matrix, num_topics=4, id2word=dictionary, passes=100)
    print(ldamodel.print_topics(num_topics=4, num_words=50))